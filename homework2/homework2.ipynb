{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('tweets_.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_preprocessed</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              tweet  \\\n",
       "id                                                             \n",
       "1     0.0   @user when a father is dysfunctional and is s...   \n",
       "2     0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "3     0.0                                bihday your majesty   \n",
       "\n",
       "                                   tweet_preprocessed  \\\n",
       "id                                                      \n",
       "1   when father is dysfunctional and is so selfish...   \n",
       "2   thanks for lyft credit cannot use cause they d...   \n",
       "3                                 bihday your majesty   \n",
       "\n",
       "                                          tweet_token  \\\n",
       "id                                                      \n",
       "1   [when, father, is, dysfunctional, and, is, so,...   \n",
       "2   [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "3                             [bihday, your, majesty]   \n",
       "\n",
       "                                 tweet_token_filtered  \\\n",
       "id                                                      \n",
       "1   [father, dysfunctional, selfish, drags, kids, ...   \n",
       "2   [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "3                                   [bihday, majesty]   \n",
       "\n",
       "                                        tweet_stemmed  \\\n",
       "id                                                      \n",
       "1   [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "2   [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "3                                   [bihday, majesti]   \n",
       "\n",
       "                                     tweet_lemmatized  \n",
       "id                                                     \n",
       "1   [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "2   [thanks, lyft, credit, use, cause, offer, whee...  \n",
       "3                                   [bihday, majesty]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = 0.9\n",
    "max_features = 5000\n",
    "stop_words = 'english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.<br>\n",
    "Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.<br>\n",
    "Исключим стоп-слова с помощью stop_words='english'. <br>\n",
    "Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_stemmed = CountVectorizer(max_df=max_df, stop_words=stop_words,max_features=max_features)\n",
    "count_vectorizer_lemmatized = CountVectorizer(max_df=max_df, stop_words=stop_words,max_features=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_count_vectorizer_stemmed = count_vectorizer_stemmed.fit_transform(df['tweet_stemmed'].astype(str).values)\n",
    "bow_count_vectorizer_lemmatized = count_vectorizer_lemmatized.fit_transform(df['tweet_lemmatized'].astype(str).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaree</th>\n",
       "      <th>aap</th>\n",
       "      <th>aare</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abc</th>\n",
       "      <th>abe</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>aboyout</th>\n",
       "      <th>...</th>\n",
       "      <th>zara</th>\n",
       "      <th>zaynmalik</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zen</th>\n",
       "      <th>zero</th>\n",
       "      <th>zionism</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaaree  aap  aare  ab  abandon  abc  abe  abil  abl  aboyout  ...  zara  \\\n",
       "0       0    0     0   0        0    0    0     0    0        0  ...     0   \n",
       "1       0    0     0   0        0    0    0     0    0        0  ...     0   \n",
       "2       0    0     0   0        0    0    0     0    0        0  ...     0   \n",
       "3       0    0     0   0        0    0    0     0    0        0  ...     0   \n",
       "4       0    0     0   0        0    0    0     0    0        0  ...     0   \n",
       "\n",
       "   zaynmalik  zelda  zen  zero  zionism  zombi  zone  zoo  zoro  \n",
       "0          0      0    0     0        0      0     0    0     0  \n",
       "1          0      0    0     0        0      0     0    0     0  \n",
       "2          0      0    0     0        0      0     0    0     0  \n",
       "3          0      0    0     0        0      0     0    0     0  \n",
       "4          0      0    0     0        0      0     0    0     0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bow_count_vectorizer_stemmed.toarray(), columns=count_vectorizer_stemmed.get_feature_names()).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaareee</th>\n",
       "      <th>aap</th>\n",
       "      <th>aaree</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboyout</th>\n",
       "      <th>abrahamhicks</th>\n",
       "      <th>...</th>\n",
       "      <th>yup</th>\n",
       "      <th>yyc</th>\n",
       "      <th>zara</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zen</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaareee  aap  aaree  ab  abandoned  abc  ability  able  aboyout  \\\n",
       "0        0    0      0   0          0    0        0     0        0   \n",
       "1        0    0      0   0          0    0        0     0        0   \n",
       "2        0    0      0   0          0    0        0     0        0   \n",
       "3        0    0      0   0          0    0        0     0        0   \n",
       "4        0    0      0   0          0    0        0     0        0   \n",
       "\n",
       "   abrahamhicks  ...  yup  yyc  zara  zelda  zen  zero  zombie  zone  zoo  \\\n",
       "0             0  ...    0    0     0      0    0     0       0     0    0   \n",
       "1             0  ...    0    0     0      0    0     0       0     0    0   \n",
       "2             0  ...    0    0     0      0    0     0       0     0    0   \n",
       "3             0  ...    0    0     0      0    0     0       0     0    0   \n",
       "4             0  ...    0    0     0      0    0     0       0     0    0   \n",
       "\n",
       "   zoro  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bow_count_vectorizer_lemmatized.toarray(), columns=count_vectorizer_lemmatized.get_feature_names()).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.<br>\n",
    "Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.<br>\n",
    "Исключим стоп-слова с помощью stop_words='english'.<br>\n",
    "Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_stemmed = TfidfVectorizer(max_df=max_df, max_features=max_features, stop_words=stop_words)\n",
    "tfidf_lemmatized = TfidfVectorizer(max_df=max_df, max_features=max_features, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_tfidf_stemmed = tfidf_stemmed.fit_transform(df['tweet_stemmed'].astype(str).values)\n",
    "bow_tfidf_lemmatized = tfidf_lemmatized.fit_transform(df['tweet_lemmatized'].astype(str).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaree</th>\n",
       "      <th>aap</th>\n",
       "      <th>aare</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abc</th>\n",
       "      <th>abe</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>aboyout</th>\n",
       "      <th>...</th>\n",
       "      <th>zara</th>\n",
       "      <th>zaynmalik</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zen</th>\n",
       "      <th>zero</th>\n",
       "      <th>zionism</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaaree  aap  aare   ab  abandon  abc  abe  abil  abl  aboyout  ...  zara  \\\n",
       "0     0.0  0.0   0.0  0.0      0.0  0.0  0.0   0.0  0.0      0.0  ...   0.0   \n",
       "1     0.0  0.0   0.0  0.0      0.0  0.0  0.0   0.0  0.0      0.0  ...   0.0   \n",
       "2     0.0  0.0   0.0  0.0      0.0  0.0  0.0   0.0  0.0      0.0  ...   0.0   \n",
       "3     0.0  0.0   0.0  0.0      0.0  0.0  0.0   0.0  0.0      0.0  ...   0.0   \n",
       "4     0.0  0.0   0.0  0.0      0.0  0.0  0.0   0.0  0.0      0.0  ...   0.0   \n",
       "\n",
       "   zaynmalik  zelda  zen  zero  zionism  zombi  zone  zoo  zoro  \n",
       "0        0.0    0.0  0.0   0.0      0.0    0.0   0.0  0.0   0.0  \n",
       "1        0.0    0.0  0.0   0.0      0.0    0.0   0.0  0.0   0.0  \n",
       "2        0.0    0.0  0.0   0.0      0.0    0.0   0.0  0.0   0.0  \n",
       "3        0.0    0.0  0.0   0.0      0.0    0.0   0.0  0.0   0.0  \n",
       "4        0.0    0.0  0.0   0.0      0.0    0.0   0.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bow_tfidf_stemmed.toarray(), columns=tfidf_stemmed.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaareee</th>\n",
       "      <th>aap</th>\n",
       "      <th>aaree</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboyout</th>\n",
       "      <th>abrahamhicks</th>\n",
       "      <th>...</th>\n",
       "      <th>yup</th>\n",
       "      <th>yyc</th>\n",
       "      <th>zara</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zen</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaareee  aap  aaree   ab  abandoned  abc  ability  able  aboyout  \\\n",
       "0      0.0  0.0    0.0  0.0        0.0  0.0      0.0   0.0      0.0   \n",
       "1      0.0  0.0    0.0  0.0        0.0  0.0      0.0   0.0      0.0   \n",
       "2      0.0  0.0    0.0  0.0        0.0  0.0      0.0   0.0      0.0   \n",
       "3      0.0  0.0    0.0  0.0        0.0  0.0      0.0   0.0      0.0   \n",
       "4      0.0  0.0    0.0  0.0        0.0  0.0      0.0   0.0      0.0   \n",
       "\n",
       "   abrahamhicks  ...  yup  yyc  zara  zelda  zen  zero  zombie  zone  zoo  \\\n",
       "0           0.0  ...  0.0  0.0   0.0    0.0  0.0   0.0     0.0   0.0  0.0   \n",
       "1           0.0  ...  0.0  0.0   0.0    0.0  0.0   0.0     0.0   0.0  0.0   \n",
       "2           0.0  ...  0.0  0.0   0.0    0.0  0.0   0.0     0.0   0.0  0.0   \n",
       "3           0.0  ...  0.0  0.0   0.0    0.0  0.0   0.0     0.0   0.0  0.0   \n",
       "4           0.0  ...  0.0  0.0   0.0    0.0  0.0   0.0     0.0   0.0  0.0   \n",
       "\n",
       "   zoro  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bow_tfidf_lemmatized.toarray(), columns=tfidf_lemmatized.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Создайте мешок слов с помощью sklearn.feature_extraction.text.HashingVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "Ограничим количество фичей, с помощью n_features = 1000.(можно изменить) <br>\n",
    "Исключим стоп-слова с помощью stop_words='english'. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer_stemmed = HashingVectorizer(stop_words=stop_words, n_features=max_features)\n",
    "hvectorizer_lemmatized = HashingVectorizer(stop_words=stop_words, n_features=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_hvectorizer_stemmed = hvectorizer_stemmed.fit_transform(df['tweet_stemmed'].astype(str).values)\n",
    "bow_hvectorizer_lemmatized = hvectorizer_lemmatized.fit_transform(df['tweet_lemmatized'].astype(str).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Проверьте ваши векторайзеры на корпусе который использовали на вебинаре, составьте таблицу метод векторизации и скор который вы получили (в методах векторизации по изменяйте параметры что бы добиться лучшего скора) обратите внимание как падает/растёт скор при уменьшении количества фичей, и изменении параметров, так же попробуйте применить к векторайзерам PCA для сокращения размерности посмотрите на качество сделайте выводы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# создаем df\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "trainDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(trainDF['text'], trainDF['label'], \n",
    "                                                                    test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=0.9, max_features=5000, stop_words='english',\n",
       "                token_pattern='\\\\w{1,}')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_origin = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                                    max_df=max_df, stop_words=stop_words, max_features=max_features)\n",
    "count_vect_origin.fit(trainDF['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_list = [count_vectorizer_stemmed, count_vectorizer_lemmatized, tfidf_stemmed, \n",
    "               tfidf_lemmatized, hvectorizer_stemmed, hvectorizer_lemmatized, count_vect_origin]\n",
    "\n",
    "vect_list_names = ['count_vectorizer_stemmed', 'count_vectorizer_lemmatized', 'tfidf_stemmed', \n",
    "               'tfidf_lemmatized', 'hvectorizer_stemmed', 'hvectorizer_lemmatized', 'count_vect_origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(X, y, X_, y_, vect_name):\n",
    "    model = linear_model.LogisticRegression()\n",
    "    Xvect = vect_name.transform(X)\n",
    "    Xvect_ = vect_name.transform(X_)\n",
    "    model.fit(Xvect, y)\n",
    "    predict = model.predict(Xvect_)\n",
    "    score = accuracy_score(y_, predict)\n",
    "    #print(score)\n",
    "    return score     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in vect_list:\n",
    "    results.append(get_score(train_x, train_y, test_x, test_y, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_score = pd.DataFrame({'model': vect_list_names, 'max_feats': max_features, 'max_df': max_df, 'score': results})\\\n",
    "#             .sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "df_score = pd.concat(map(pd.read_csv, glob.glob(os.path.join('', \"*.csv\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<p>\n",
    "    Было лень создавать функцию для созднаний векторайзеров с различными параметрами, поэтому просто был запущен ноутбук несколько раз, результаты выгружены в CSV, после чего всё собрано в один датафрейм <br>\n",
    "Мы видим следующие закономерности: <br>\n",
    "1) Лемматизация в целом отрабатывает лучше. <br>\n",
    "2) Векторайзеры, построенные на твитах отрабатывают соизмеримо с тем, как отработал векторайзер, обученный непосредственно на данном датасете. Это можно объяснить стилистической близостью твитов и отзывов, которые оставляют пользователи. Но если бы мы обучили их на твитах, а пытались бы использовать совершенно иных текстах (официальные письма, тех. документация с большим использованием терминов) - возникла бы проблема из-за совершенно разного состава этих мешком слов. <br>\n",
    "3) Размер словаря увеличивает точность, но и существенно повышает вычислительную сложность\n",
    "    </p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>model</th>\n",
       "      <th>max_feats</th>\n",
       "      <th>max_df</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>count_vect_origin</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.841000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>count_vect_origin</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.841000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>count_vect_origin</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.823667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>count_vect_origin</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.823667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>hvectorizer_lemmatized</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>hvectorizer_stemmed</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>hvectorizer_lemmatized</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>hvectorizer_stemmed</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>count_vectorizer_lemmatized</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.811667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>count_vectorizer_lemmatized</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.811667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.806667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>tfidf_lemmatized</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.806667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>count_vectorizer_lemmatized</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.797667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>count_vectorizer_lemmatized</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.797667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>hvectorizer_stemmed</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.785333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>hvectorizer_lemmatized</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.785333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>hvectorizer_lemmatized</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.785333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>hvectorizer_stemmed</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.785333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>tfidf_stemmed</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>tfidf_stemmed</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>count_vectorizer_stemmed</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.751667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>count_vectorizer_stemmed</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.751667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>tfidf_stemmed</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>tfidf_stemmed</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>count_vectorizer_stemmed</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.739667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>count_vectorizer_stemmed</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.739667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                        model  max_feats  max_df     score\n",
       "0           6            count_vect_origin       5000     0.7  0.841000\n",
       "0           6            count_vect_origin       5000     0.9  0.841000\n",
       "1           3             tfidf_lemmatized       5000     0.9  0.825000\n",
       "1           3             tfidf_lemmatized       5000     0.7  0.825000\n",
       "0           6            count_vect_origin       2000     0.9  0.823667\n",
       "0           6            count_vect_origin       2000     0.7  0.823667\n",
       "3           5       hvectorizer_lemmatized       5000     0.7  0.820000\n",
       "2           4          hvectorizer_stemmed       5000     0.7  0.820000\n",
       "3           5       hvectorizer_lemmatized       5000     0.9  0.820000\n",
       "2           4          hvectorizer_stemmed       5000     0.9  0.820000\n",
       "4           1  count_vectorizer_lemmatized       5000     0.9  0.811667\n",
       "4           1  count_vectorizer_lemmatized       5000     0.7  0.811667\n",
       "1           3             tfidf_lemmatized       2000     0.9  0.806667\n",
       "1           3             tfidf_lemmatized       2000     0.7  0.806667\n",
       "2           1  count_vectorizer_lemmatized       2000     0.7  0.797667\n",
       "2           1  count_vectorizer_lemmatized       2000     0.9  0.797667\n",
       "3           4          hvectorizer_stemmed       2000     0.9  0.785333\n",
       "4           5       hvectorizer_lemmatized       2000     0.9  0.785333\n",
       "4           5       hvectorizer_lemmatized       2000     0.7  0.785333\n",
       "3           4          hvectorizer_stemmed       2000     0.7  0.785333\n",
       "5           2                tfidf_stemmed       5000     0.9  0.766000\n",
       "5           2                tfidf_stemmed       5000     0.7  0.766000\n",
       "6           0     count_vectorizer_stemmed       5000     0.9  0.751667\n",
       "6           0     count_vectorizer_stemmed       5000     0.7  0.751667\n",
       "5           2                tfidf_stemmed       2000     0.7  0.747000\n",
       "5           2                tfidf_stemmed       2000     0.9  0.747000\n",
       "6           0     count_vectorizer_stemmed       2000     0.9  0.739667\n",
       "6           0     count_vectorizer_stemmed       2000     0.7  0.739667"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<p>\n",
    "Попробуем применить понижение размерности к лучшим вариантам\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = TruncatedSVD(500) # PCA не отработал на разряженной матрице"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = count_vect_origin.transform(train_x)\n",
    "X_ = count_vect_origin.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pca.fit_transform(X)\n",
    "X_ = pca.transform(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8236666666666667"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y, model.predict(X_))\n",
    "# 0.841000 на оригинальной матрице из 5000 признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_lemmatized.transform(train_x)\n",
    "X_ = tfidf_lemmatized.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166666666666667"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pca.fit_transform(X)\n",
    "X_ = pca.transform(X_)\n",
    "model.fit(X, train_y)\n",
    "accuracy_score(test_y, model.predict(X_))\n",
    "# 0.825000  на tf idf матрице из 5000 признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<p>\n",
    "Существенное снижение признакового пространства понижает качество модели, но не фатально, качество сопоставимо. Вместо 5 000 признаков мы использовали 500.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
