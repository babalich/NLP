{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkpcHsV8RWHA"
   },
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAQBOJRARev7"
   },
   "source": [
    "**Написать теггер на данных с руским языком**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_16J0ER8WOJx"
   },
   "source": [
    "## загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4611,
     "status": "ok",
     "timestamp": 1617782512502,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "yPRx8Cu_RDY1",
    "outputId": "9595e8e4-9731-4cfe-a67f-cfb0d9759cd4"
   },
   "outputs": [],
   "source": [
    "#!pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1617782515586,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "9wgL-33mWUyZ"
   },
   "outputs": [],
   "source": [
    "import pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1617782632041,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "vXxwW9NzW570"
   },
   "outputs": [],
   "source": [
    "#!mkdir datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2783,
     "status": "ok",
     "timestamp": 1617782637610,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "tpwgA3svWiRw",
    "outputId": "9d4e2aa6-bcc7-4793-f85d-2ce708f88ff8"
   },
   "outputs": [],
   "source": [
    "# !wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
    "# !wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 25059,
     "status": "ok",
     "timestamp": 1617782687090,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "Oymo30RBWjjl"
   },
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 609,
     "status": "ok",
     "timestamp": 1617782704463,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "XBzFe82cXGNK",
    "outputId": "3c13d3e6-498a-47bc-e729-d2f953cddfb6"
   },
   "outputs": [],
   "source": [
    "# for sent in full_train[1:2]:\n",
    "#     for token in sent:\n",
    "#         print(token.form, ' _ ', token.upos)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OshO48XLXQar"
   },
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form.lower(), token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form.lower(), token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_sent_train.append([token.form.lower() for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form.lower() for token in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8819465507363596"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8883075523202911"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "bigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8874734607218684"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "trigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838826542648199"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NN') \n",
    "tag = backoff_tagger(fdata_test,  \n",
    "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(fdata_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "    Подозрительно высокие результаты гораздо выше чем в методичке, посмотрим распределение тегов\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_pairs_test = []\n",
    "for sent in fdata_test[:]:\n",
    "    fdata_pairs_test.extend(i for i in sent)\n",
    "\n",
    "tags = [tag for (word, tag) in fdata_pairs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NOUN': 27974, 'PUNCT': 22694, 'VERB': 13078, 'ADJ': 11222, 'ADP': 10585, 'ADV': 6165, 'PRON': 5598, 'PROPN': 4438, 'CCONJ': 4410, 'PART': 3877, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "    Всё перепроверил. Просто в методичке мы сразу вызвали bigram tagger, а в качестве backoff указали юниграм, обученный на другом датасете :)\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. написать свой теггер как на занятии, но улучшить попробовать разные векторайзеры, добавить знание не только букв и слов но и совместно объединить эти признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dj4tV8ytXTry"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer='char', max_features=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10, random_state=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=10)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6128972466552084"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "    0.612 - скор для бейзлайна (держим в голове, что у нас всего 30 признаков)\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = CountVectorizer(ngram_range=(1,5), analyzer='word', max_features=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word = word_vectorizer.fit_transform(train_tok)\n",
    "X_test_word = word_vectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3118070299598962"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1 = LogisticRegression(random_state=0, max_iter=10)\n",
    "lr1.fit(X_train_word, train_enc_labels)\n",
    "pred1 = lr1.predict(X_test_word)\n",
    "accuracy_score(test_enc_labels, pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "    анализировать только по словам не очень, попробуем объединить два подхода <br>\n",
    "    При использовании только vectorizer = CountVectorizer(ngram_range=(1, 9), analyzer='char') скор 0.79\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 9), analyzer='char')\n",
    "X_train_c = vectorizer.fit_transform(train_tok)\n",
    "X_test_c = vectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = CountVectorizer(ngram_range=(1,5), analyzer='word')\n",
    "X_train_w = word_vectorizer.fit_transform(train_tok)\n",
    "X_test_w = word_vectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack((X_train_c, X_train_w))\n",
    "X_test = hstack((X_test_c, X_test_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((871526, 692910), (871526, 593425), (871526, 99485))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train_c.shape, X_train_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9126647120277693"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=25)\n",
    "lr.fit(X_train, train_enc_labels)\n",
    "pred = lr.predict(X_test)\n",
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. вместо векторайзеров взять эмбединги попробовать (word2vec и fasttext по желанию дополнительно можно взять tf.keras.layers.Embedding)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word2vec = fdata_sent_train\n",
    "train_word2vec.extend(fdata_sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3850133, 4951090)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model = gensim.models.Word2Vec(min_count=1, negative=20, sg=1, window=2)\n",
    "w2v_model.build_vocab(train_word2vec)\n",
    "w2v_model.train(train_word2vec, epochs=5, total_examples=w2v_model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = [w2v_model.wv[word] for word in train_tok]\n",
    "X_test_w2v = [w2v_model.wv[word] for word in test_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7433019917096351"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=25)\n",
    "lr.fit(X_train_w2v, train_enc_labels)\n",
    "pred = lr.predict(X_test_w2v)\n",
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "0.74 приемлимый скор для минималистичной модели (эмбединги учили сами на маленьком корпусе)\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = gensim.models.FastText(train_word2vec, min_count=1, negative=20, sg=1, window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ft = [ft_model.wv[word] for word in train_tok]\n",
    "X_test_ft = [ft_model.wv[word] for word in test_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8639251171098304"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=25)\n",
    "lr.fit(X_train_ft, train_enc_labels)\n",
    "pred = lr.predict(X_test_ft)\n",
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. взять не только эмбединги каждого слова, но и взять соседей, т.е. информацию о соседях количество соседей выбрать самим (узнать наилучшее количество соседей)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.8631837023556769\n",
      "2 0.8636470865770229\n",
      "3 0.861734573518013\n",
      "4 0.8586762376571294\n",
      "5 0.8587436389984161\n",
      "6 0.855036565227648\n",
      "7 0.8500320156371112\n",
      "8 0.8508324065648907\n",
      "9 0.8516159471573485\n"
     ]
    }
   ],
   "source": [
    "windows = [1,2,3,4,5,6,7,8,9]\n",
    "for i in windows:\n",
    "    ft_model = gensim.models.FastText(train_word2vec, min_count=1, negative=20, sg=1, window=i)\n",
    "    X_train_ft = [ft_model.wv[word] for word in train_tok]\n",
    "    X_test_ft = [ft_model.wv[word] for word in test_tok]\n",
    "    lr.fit(X_train_ft, train_enc_labels)\n",
    "    pred = lr.predict(X_test_ft)\n",
    "    print(i, accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. сравнить все реализованные методы сделать выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Лучше всего отработали комбинации теггеров. <br>\n",
    "На самом деле все подходы можно считать работающими, просто результаты получились не идеальными, потому что для обучения мы использовали небольшой корпус.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cINqgGpKXURp"
   },
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCM0drjKXYet"
   },
   "source": [
    "много дополнительных датасетов на русском языке\n",
    "\n",
    "https://natasha.github.io/corus/  \n",
    "https://github.com/natasha/corus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUOg4C8sZNpw"
   },
   "source": [
    "мы будем использовать данные http://ai-center.botik.ru/Airec/index.php/ru/collections/28-persons-1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzi6ApNLZg6X"
   },
   "source": [
    "**Проверить насколько хорошо работает NER**\n",
    "\n",
    "1. взять нер из nltk\n",
    "2. проверить deeppavlov\n",
    "3. написать свой нер попробовать разные подходы (с доп информацией без) так же с учётом соседей и без них\n",
    "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP1LgaNUtaOz"
   },
   "source": [
    "при обучении своего нера незабудьте разделить выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3659,
     "status": "ok",
     "timestamp": 1617783503855,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "Qg6tcss2Zhp9",
    "outputId": "7fc2886a-f7d9-46d4-afae-a7ea0d7e7ac4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\pavel\\anaconda3\\envs\\tensorflow\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: corus in c:\\users\\pavel\\anaconda3\\envs\\tensorflow\\lib\\site-packages (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install corus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1617783563611,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "hrc5ocDkaS1e"
   },
   "outputs": [],
   "source": [
    "import corus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1617787470947,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "fmJr9tubbTVk"
   },
   "outputs": [],
   "source": [
    "path = 'Persons-1000.zip'\n",
    "records = corus.persons.load_persons(path)\n",
    "rec = next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 761,
     "status": "ok",
     "timestamp": 1617787495791,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "tzQutiCephkx",
    "outputId": "023aacfb-d343-418c-f441-be0100abc605"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PersonsMarkup(\n",
       "    text='Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\\r\\n\\r\\n05/08/2008 10:32\\r\\n\\r\\nМОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\\r\\n\\r\\n\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\\r\\n\\r\\nПо сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\\r\\n\\r\\nКомиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\\r\\n\\r\\n\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\\r\\n\\r\\nКомиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.',\n",
       "    spans=[PersonsSpan(\n",
       "         id=1,\n",
       "         start=362,\n",
       "         stop=398,\n",
       "         value='ТОМАС ХАММАРБЕРГ (THOMAS HAMMARBERG)'\n",
       "     ),\n",
       "     PersonsSpan(\n",
       "         id=2,\n",
       "         start=624,\n",
       "         stop=634,\n",
       "         value='ХАММАРБЕРГ'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "executionInfo": {
     "elapsed": 763,
     "status": "ok",
     "timestamp": 1617787498774,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "utanZXahn-2I",
    "outputId": "376ce6d7-5861-4468-db98-6b9c0ef076d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\\r\\n\\r\\n05/08/2008 10:32\\r\\n\\r\\nМОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\\r\\n\\r\\n\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\\r\\n\\r\\nПо сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\\r\\n\\r\\nКомиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\\r\\n\\r\\n\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\\r\\n\\r\\nКомиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1617787499993,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "QBaxNLbQoucW",
    "outputId": "314360eb-a650-45ee-e8f4-5b0329b1e2a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PersonsSpan(\n",
       "     id=1,\n",
       "     start=362,\n",
       "     stop=398,\n",
       "     value='ТОМАС ХАММАРБЕРГ (THOMAS HAMMARBERG)'\n",
       " ),\n",
       " PersonsSpan(\n",
       "     id=2,\n",
       "     start=624,\n",
       "     stop=634,\n",
       "     value='ХАММАРБЕРГ'\n",
       " )]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oRyOTnGtVSw"
   },
   "source": [
    "### 1. взять нер из nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "hv4GhkdztVNC"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Thomas Hammarberg', 'PERSON'),\n",
       " ('Комиссар', 'PERSON'),\n",
       " ('МОСКВА', 'ORGANIZATION'),\n",
       " ('РИА Новости', 'ORGANIZATION'),\n",
       " ('СЕ', 'ORGANIZATION'),\n",
       " ('Совета Европы', 'PERSON'),\n",
       " ('Хаммарберг', 'PERSON')}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(rec.text))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Ошибочно попал \"совет европы\". Комиссар вполне сойдет за персону\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. написать свой нер попробовать разные подходы (с доп информацией без) так же с учётом соседей и без них"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "не успеваю нормально доделать до дедлайна. Оценку можно ставить за то, что есть\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PersonsMarkup(\n",
       "    text='Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\\r\\n\\r\\n05/08/2008 10:32\\r\\n\\r\\nМОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\\r\\n\\r\\n\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\\r\\n\\r\\nПо сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\\r\\n\\r\\nКомиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\\r\\n\\r\\n\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\\r\\n\\r\\nКомиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.',\n",
       "    spans=[PersonsSpan(\n",
       "         id=1,\n",
       "         start=362,\n",
       "         stop=398,\n",
       "         value='ТОМАС ХАММАРБЕРГ (THOMAS HAMMARBERG)'\n",
       "     ),\n",
       "     PersonsSpan(\n",
       "         id=2,\n",
       "         start=624,\n",
       "         stop=634,\n",
       "         value='ХАММАРБЕРГ'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Томас Хаммарберг (Thomas Hammarberg)'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.text[362:398]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['томас', 'хаммарберг', '(', 'thomas', 'hammarberg', ')']\n",
      "['хаммарберг']\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7785,
     "status": "ok",
     "timestamp": 1617788198832,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "2kd-emBao1u-",
    "outputId": "a31344d7-177b-4b5f-b49e-4d4201f529ff"
   },
   "outputs": [],
   "source": [
    "# # установка deeppavlov\n",
    "\n",
    "# !pip uninstall -y tensorflow tensorflow-gpu\n",
    "# !pip install numpy scipy librosa unidecode inflect librosa transformers\n",
    "# !pip install deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1218,
     "status": "ok",
     "timestamp": 1617789353363,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "KY96lqBzsZJ_"
   },
   "outputs": [],
   "source": [
    "#!python -m deeppavlov install squad_bert\n",
    "#!python -m deeppavlov install ner_ontonotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 664,
     "status": "ok",
     "timestamp": 1617788363229,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "6KE7tpVWs1b7"
   },
   "outputs": [],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov import configs, build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74142,
     "status": "ok",
     "timestamp": 1617789434005,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "wsegbgCbrzy_",
    "outputId": "98fbfd4e-0fff-433a-9c24-2fa56ab7d241"
   },
   "outputs": [],
   "source": [
    "deeppavlov_ner = build_model(configs.ner.ner_bert_ent_and_type_rus, download=True)\n",
    "rus_document = \"Нью-Йорк, США, 30 апреля 2020, 01:01 — REGNUM В администрации президента США Дональда Трампа планируют пройти все этапы создания вакцины от коронавируса в ускоренном темпе и выпустить 100 млн доз до конца 2020 года, передаёт агентство Bloomberg со ссылкой на осведомлённые источники\"\n",
    "deeppavlov_ner([rus_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQ1phPKisJMz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNalt7JF1MCI8wcdUXnuHad",
   "name": "HW5-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
